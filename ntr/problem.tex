Machine learning technologies nowadays are widely used in various aspects of 
space exploration, including terrain classification, automatic target selection, 
and downlink data prioritization. Designing and implementing machine learning 
systems is usually an iterative process which consists of (1) labeled data 
acquisition, (2) training, and (3) evaluation. Analyzing and reviewing data is 
a key and essential component that every machine learning system has to handle 
in order to achieve high performance. For example, tens of thousands of data 
points need to be analyzed and labeled for training and evaluation; errors in 
classification results need to be reviewed in evaluation, and corrections need 
to be submitted back to the system to re-train the model. Given the massive 
amount of data associated with machine learning systems, interpretability becomes 
more and more challenging. The burning questions arise: Are there outliers in 
training data? Why the model confuses certain classes and always incorrectly 
classify them? Is it worth spending time and computational resource to fix a 
problem in the labels?
